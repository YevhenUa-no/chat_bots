import streamlit as st

from openai import OpenAI

from streamlit_js_eval import streamlit_js_eval

from streamlit_mic_recorder import mic_recorder

import os

import base64



# Setting up the Streamlit page configuration

st.set_page_config(page_title="Streamlit Interview Bot", page_icon="ğŸ’¬")

st.title("Interview Bot")



# Initialize session state variables

if "setup_complete" not in st.session_state:

Â  Â  st.session_state.setup_complete = False

if "user_message_count" not in st.session_state:

Â  Â  st.session_state.user_message_count = 0

if "feedback_shown" not in st.session_state:

Â  Â  st.session_state.feedback_shown = False

if "chat_complete" not in st.session_state:

Â  Â  st.session_state.chat_complete = False

if "messages" not in st.session_state:

Â  Â  st.session_state.messages = []

# New session state for controlling the flow after assistant response

if "awaiting_user_action" not in st.session_state:

Â  Â  st.session_state.awaiting_user_action = False

# Session state to hold the AI's last spoken text and audio path

if "current_ai_response_text" not in st.session_state:

Â  Â  st.session_state.current_ai_response_text = ""

if "current_ai_audio_path" not in st.session_state:

Â  Â  st.session_state.current_ai_audio_path = ""

# Session state for feedback audio file path

if "feedback_audio_path" not in st.session_state:

Â  Â  st.session_state.feedback_audio_path = ""





# Session state variables for initial personal information audio transcriptions

if "name_audio_transcription" not in st.session_state:

Â  Â  st.session_state.name_audio_transcription = ""

if "experience_audio_transcription" not in st.session_state:

Â  Â  st.session_state.experience_audio_transcription = ""

if "skills_audio_transcription" not in st.session_state:

Â  Â  st.session_state.skills_audio_transcription = ""



# New session state for temporary voice input during the chat interview

if "current_chat_voice_input" not in st.session_state:

Â  Â  st.session_state.current_chat_voice_input = ""



# Helper functions to update session state

def complete_setup():

Â  Â  st.session_state.setup_complete = True



def show_feedback():

Â  Â  st.session_state.feedback_shown = True



# --- Convert text to audio ---

def text_to_audio(client, text, audio_path, voice_type="alloy"):

Â  Â  try:

Â  Â  Â  Â  response = client.audio.speech.create(model="tts-1", voice=voice_type, input=text)

Â  Â  Â  Â  response.stream_to_file(audio_path)

Â  Â  except Exception as e:

Â  Â  Â  Â  st.error(f"Error converting text to audio: {e}")



# --- Autoplay audio ---

def auto_play_audio(audio_file_path):

Â  Â  if os.path.exists(audio_file_path):

Â  Â  Â  Â  with open(audio_file_path, "rb") as audio_file:

Â  Â  Â  Â  Â  Â  audio_bytes = audio_file.read()

Â  Â  Â  Â  base64_audio = base64.b64encode(audio_bytes).decode("utf-8")

Â  Â  Â  Â  audio_html = f'<audio src="data:audio/mp3;base64,{base64_audio}" controls autoplay></audio>'

Â  Â  Â  Â  st.markdown(audio_html, unsafe_allow_html=True)

Â  Â  else:

Â  Â  Â  Â  st.error(f"Error: Audio file not found at {audio_file_path}")





# Function to handle audio recording and transcription for initial setup

def handle_audio_input_setup(slot_name, key):

Â  Â  mic_recorder_output = mic_recorder(

Â  Â  Â  Â  start_prompt="ğŸ™ï¸ Speak",

Â  Â  Â  Â  stop_prompt="â¹ï¸ Stop",

Â  Â  Â  Â  just_once=True,

Â  Â  Â  Â  use_container_width=True,

Â  Â  Â  Â  key=key

Â  Â  )



Â  Â  if mic_recorder_output:

Â  Â  Â  Â  client = OpenAI(api_key=st.secrets["OPENAI_API_KEY"])

Â  Â  Â  Â  audio_bytes = mic_recorder_output['bytes']

Â  Â  Â  Â  if audio_bytes:

Â  Â  Â  Â  Â  Â  temp_audio_file_path = f"audio_{slot_name}.webm"

Â  Â  Â  Â  Â  Â  with open(temp_audio_file_path, "wb") as f:

Â  Â  Â  Â  Â  Â  Â  Â  f.write(audio_bytes)



Â  Â  Â  Â  Â  Â  with st.spinner(f"Transcribing {slot_name}..."):

Â  Â  Â  Â  Â  Â  Â  Â  audio_file = open(temp_audio_file_path, "rb")

Â  Â  Â  Â  Â  Â  Â  Â  transcript = client.audio.transcriptions.create(

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  model="whisper-1",

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  file=audio_file

Â  Â  Â  Â  Â  Â  Â  Â  )

Â  Â  Â  Â  Â  Â  Â  Â  audio_file.close()

Â  Â  Â  Â  Â  Â  Â  Â  os.remove(temp_audio_file_path)



Â  Â  Â  Â  Â  Â  setattr(st.session_state, f"{slot_name}_audio_transcription", transcript.text)

Â  Â  Â  Â  Â  Â  st.write(f"**Transcription for {slot_name}:** {transcript.text}")

Â  Â  Â  Â  Â  Â  st.session_state[slot_name] = transcript.text

Â  Â  Â  Â  Â  Â  st.rerun()

Â  Â  return getattr(st.session_state, f"{slot_name}_audio_transcription")





# --- Setup Stage ---

if not st.session_state.setup_complete:

Â  Â  st.subheader('Personal Information')



Â  Â  # Initialize session state for personal information

Â  Â  if "name" not in st.session_state:

Â  Â  Â  Â  st.session_state["name"] = ""

Â  Â  if "experience" not in st.session_state:

Â  Â  Â  Â  st.session_state["experience"] = ""

Â  Â  if "skills" not in st.session_state:

Â  Â  Â  Â  st.session_state["skills"] = ""

Â  Â  if "job_post" not in st.session_state:

Â  Â  Â  Â  st.session_state["job_post"] = ""





Â  Â  # Option to input via text or voice

Â  Â  input_method = st.radio("How would you like to provide your information?", ("Type", "Speak"), index=0, key="input_method_radio") #If you want to add Speak ad default option for Personal Data input - change to index 1Â 



Â  Â  if input_method == "Type":

Â  Â  Â  Â  st.session_state["name"] = st.text_input(label="Name", value=st.session_state["name"], placeholder="Enter your name", max_chars=40, key="name_text_input")

Â  Â  Â  Â  st.session_state["experience"] = st.text_area(label="Experience", value=st.session_state["experience"], placeholder="Describe your experience", max_chars=200, key="experience_text_input")

Â  Â  Â  Â  st.session_state["skills"] = st.text_area(label="Skills", value=st.session_state["skills"], placeholder="List your skills", max_chars=200, key="skills_text_input")

Â  Â  else: # input_method == "Speak"

Â  Â  Â  Â  st.write("### Name")

Â  Â  Â  Â  current_name_transcription = handle_audio_input_setup("name", "mic_recorder_name")

Â  Â  Â  Â  st.session_state["name"] = st.text_input(

Â  Â  Â  Â  Â  Â  label="Name (from audio or manual edit)",

Â  Â  Â  Â  Â  Â  value=st.session_state["name"] if st.session_state["name"] else current_name_transcription,

Â  Â  Â  Â  Â  Â  placeholder="Enter your name or speak it", max_chars=40,

Â  Â  Â  Â  Â  Â  key="name_input_final"

Â  Â  Â  Â  )

Â  Â  Â  Â  st.markdown("---")



Â  Â  Â  Â  st.write("### Experience")

Â  Â  Â  Â  current_experience_transcription = handle_audio_input_setup("experience", "mic_recorder_experience")

Â  Â  Â  Â  st.session_state["experience"] = st.text_area(

Â  Â  Â  Â  Â  Â  label="Experience (from audio or manual edit)",

Â  Â  Â  Â  Â  Â  value=st.session_state["experience"] if st.session_state["experience"] else current_experience_transcription,

Â  Â  Â  Â  Â  Â  placeholder="Describe your experience or speak it", max_chars=200,

Â  Â  Â  Â  Â  Â  key="experience_input_final"

Â  Â  Â  Â  )

Â  Â  Â  Â  st.markdown("---")



Â  Â  Â  Â  st.write("### Skills")

Â  Â  Â  Â  current_skills_transcription = handle_audio_input_setup("skills", "mic_recorder_skills")

Â  Â  Â  Â  st.session_state["skills"] = st.text_area(

Â  Â  Â  Â  Â  Â  label="Skills (from audio or manual edit)",

Â  Â  Â  Â  Â  Â  value=st.session_state["skills"] if st.session_state["skills"] else current_skills_transcription,

Â  Â  Â  Â  Â  Â  placeholder="List your skills or speak them", max_chars=200,

Â  Â  Â  Â  Â  Â  key="skills_input_final"

Â  Â  Â  Â  )

Â  Â  Â  Â  st.markdown("---")



Â  Â  # Company and Position Section

Â  Â  st.subheader('Company and Position')



Â  Â  # Initialize session state for company and position information

Â  Â  if "position" not in st.session_state:

Â  Â  Â  Â  st.session_state["position"] = ""

Â  Â  if "company" not in st.session_state:

Â  Â  Â  Â  st.session_state["company"] = ""



Â  Â  # Replace dropdowns and radio buttons with text inputs

Â  Â  st.session_state["company"] = st.text_input(

Â  Â  Â  Â  label="Company Name",

Â  Â  Â  Â  value=st.session_state["company"],

Â  Â  Â  Â  placeholder="e.g., Google",

Â  Â  Â  Â  key="company_text_input"

Â  Â  )



Â  Â  st.session_state["position"] = st.text_input(

Â  Â  Â  Â  label="Position Title",

Â  Â  Â  Â  value=st.session_state["position"],

Â  Â  Â  Â  placeholder="e.g., Senior Software Engineer",

Â  Â  Â  Â  key="position_text_input"

Â  Â  )



Â  Â  st.session_state["job_post"] = st.text_area(

Â  Â  Â  Â  label="Job Post Description (Optional)",

Â  Â  Â  Â  value=st.session_state["job_post"],

Â  Â  Â  Â  placeholder="Paste the job description here to help the interviewer ask more relevant questions.",

Â  Â  Â  Â  max_chars=2000,

Â  Â  Â  Â  key="job_post_text_area"

Â  Â  )





Â  Â  # Button to complete setup

Â  Â  if st.button("Start Interview", on_click=complete_setup, key="start_interview_button"):

Â  Â  Â  Â  if not (st.session_state["name"] or st.session_state["experience"] or st.session_state["skills"]):

Â  Â  Â  Â  Â  Â  st.warning("Please provide your personal information before starting the interview.")

Â  Â  Â  Â  Â  Â  st.session_state.setup_complete = False

Â  Â  Â  Â  else:

Â  Â  Â  Â  Â  Â  st.write("Setup complete. Starting interview...")

Â  Â  Â  Â  Â  Â  streamlit_js_eval(js_expressions="parent.window.location.reload()")





# --- Interview Phase ---

if st.session_state.setup_complete and not st.session_state.feedback_shown and not st.session_state.chat_complete:



Â  Â  st.info(

Â  Â  """

Â  Â  **Interview Started!** Please introduce yourself.

Â  Â  """,

Â  Â  icon="ğŸ¤",

Â  Â  )



Â  Â  # Initialize OpenAI client

Â  Â  client = OpenAI(api_key=st.secrets["OPENAI_API_KEY"])



Â  Â  # Setting OpenAI model if not already initialized

Â  Â  if "openai_model" not in st.session_state:

Â  Â  Â  Â  st.session_state["openai_model"] = "gpt-4o"



Â  Â  # Initializing the system prompt for the chatbot

Â  Â  if not st.session_state.messages:

Â  Â  Â  Â  # Construct the system prompt with the new job_post information

Â  Â  Â  Â  system_prompt_content = (

Â  Â  Â  Â  Â  Â  f"You are an HR executive that interviews an interviewee called {st.session_state['name']} "

Â  Â  Â  Â  Â  Â  f"with experience: {st.session_state['experience']} and skills: {st.session_state['skills']}. "

Â  Â  Â  Â  Â  Â  f"You should interview him for the position {st.session_state['position']} "

Â  Â  Â  Â  Â  Â  f"at the company {st.session_state['company']}."

Â  Â  Â  Â  )

Â  Â  Â  Â  # Conditionally add the job post information to the prompt

Â  Â  Â  Â  if st.session_state["job_post"]:

Â  Â  Â  Â  Â  Â  system_prompt_content += f" The job post description is as follows: {st.session_state['job_post']}."



Â  Â  Â  Â  st.session_state.messages = [{

Â  Â  Â  Â  Â  Â  "role": "system",

Â  Â  Â  Â  Â  Â  "content": system_prompt_content

Â  Â  Â  Â  }]



Â  Â  # --- Display Past Messages (Text Only for History) ---

Â  Â  st.subheader("Conversation History")

Â  Â  for message in st.session_state.messages:

Â  Â  Â  Â  if message["role"] == "user":

Â  Â  Â  Â  Â  Â  st.markdown(f"**You:** {message['content']}")

Â  Â  Â  Â  elif message["role"] == "assistant" and "content" in message:

Â  Â  Â  Â  Â  Â  st.markdown(f"**Interviewer:** {message['content']}")

Â  Â  st.markdown("---")



Â  Â  # --- Interview Turn Logic ---



Â  Â  # Case 1: Assistant has just responded, waiting for user to click "Continue"

Â  Â  if st.session_state.awaiting_user_action:

Â  Â  Â  Â  st.info("Please listen to the interviewer's response and click 'Next Question' when you're ready.")



Â  Â  Â  Â  if st.session_state.current_ai_audio_path and os.path.exists(st.session_state.current_ai_audio_path):

Â  Â  Â  Â  Â  Â  auto_play_audio(st.session_state.current_ai_audio_path)

Â  Â  Â  Â  st.write(f"**Interviewer:** {st.session_state.current_ai_response_text}")



Â  Â  Â  Â  if st.button("Next Question", key="continue_interview_button"):

Â  Â  Â  Â  Â  Â  st.session_state.awaiting_user_action = False

Â  Â  Â  Â  Â  Â  st.session_state.user_message_count += 1

Â  Â  Â  Â  Â  Â  st.rerun()



Â  Â  # Case 2: Ready for user input (either initial turn or after clicking "Continue")

Â  Â  elif st.session_state.user_message_count < 5:

Â  Â  Â  Â  st.subheader(f"Your Turn (Question {st.session_state.user_message_count + 1} of 5)")

Â  Â  Â  Â  mic_recorder_chat_output = mic_recorder(

Â  Â  Â  Â  Â  Â  start_prompt="ğŸ™ï¸ Speak your answer",

Â  Â  Â  Â  Â  Â  stop_prompt="â¹ï¸ Stop Recording",

Â  Â  Â  Â  Â  Â  just_once=True,

Â  Â  Â  Â  Â  Â  use_container_width=True,

Â  Â  Â  Â  Â  Â  key=f"mic_recorder_chat_turn_{st.session_state.user_message_count}"

Â  Â  Â  Â  )



Â  Â  Â  Â  if mic_recorder_chat_output and mic_recorder_chat_output['bytes']:

Â  Â  Â  Â  Â  Â  audio_bytes = mic_recorder_chat_output['bytes']

Â  Â  Â  Â  Â  Â  temp_audio_file_path = "chat_audio_response.webm"

Â  Â  Â  Â  Â  Â  with open(temp_audio_file_path, "wb") as f:

Â  Â  Â  Â  Â  Â  Â  Â  f.write(audio_bytes)



Â  Â  Â  Â  Â  Â  with st.spinner("Transcribing your answer..."):

Â  Â  Â  Â  Â  Â  Â  Â  audio_file = open(temp_audio_file_path, "rb")

Â  Â  Â  Â  Â  Â  Â  Â  transcribed_text = client.audio.transcriptions.create(

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  model="whisper-1",

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  file=audio_file

Â  Â  Â  Â  Â  Â  Â  Â  ).text

Â  Â  Â  Â  Â  Â  Â  Â  audio_file.close()

Â  Â  Â  Â  Â  Â  Â  Â  os.remove(temp_audio_file_path)



Â  Â  Â  Â  Â  Â  st.session_state.current_chat_voice_input = transcribed_text

Â  Â  Â  Â  Â  Â  st.rerun()



Â  Â  Â  Â  user_prompt_input = st.text_area(

Â  Â  Â  Â  Â  Â  "Your answer:",

Â  Â  Â  Â  Â  Â  value=st.session_state.current_chat_voice_input,

Â  Â  Â  Â  Â  Â  placeholder="Type your response here or speak it...",

Â  Â  Â  Â  Â  Â  max_chars=1000,

Â  Â  Â  Â  Â  Â  key=f"chat_text_area_{st.session_state.user_message_count}"

Â  Â  Â  Â  )



Â  Â  Â  Â  if st.button("Send Answer", key=f"send_answer_button_{st.session_state.user_message_count}"):

Â  Â  Â  Â  Â  Â  if user_prompt_input:

Â  Â  Â  Â  Â  Â  Â  Â  st.session_state.messages.append({"role": "user", "content": user_prompt_input})

Â  Â  Â  Â  Â  Â  Â  Â  st.markdown(f"**You:** {user_prompt_input}")



Â  Â  Â  Â  Â  Â  Â  Â  st.session_state.current_chat_voice_input = ""



Â  Â  Â  Â  Â  Â  Â  Â  if st.session_state.user_message_count < 5:

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  with st.spinner("Interviewer is thinking..."):

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  stream = client.chat.completions.create(

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  model=st.session_state["openai_model"],

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  messages=[

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  {"role": m["role"], "content": m["content"]}

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  for m in st.session_state.messages

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  ],

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  stream=True,

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  )

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  response_text = ""

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  response_placeholder = st.empty()

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  for chunk in stream:

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  if chunk.choices[0].delta.content is not None:

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  response_text += chunk.choices[0].delta.content

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  response_placeholder.markdown(f"**Interviewer:** {response_text}")



Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  speech_file_path = f"assistant_response_{st.session_state.user_message_count}.mp3"

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  try:

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  text_to_audio(client, response_text, speech_file_path, voice_type="alloy")

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â Â 

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  st.session_state.messages.append({"role": "assistant", "content": response_text, "audio_file_path": speech_file_path})

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  st.session_state.current_ai_response_text = response_text

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  st.session_state.current_ai_audio_path = speech_file_path

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â Â 

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  st.session_state.awaiting_user_action = True

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  st.rerun()

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  except Exception as e:

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  st.error(f"Error generating or playing speech: {e}. Falling back to text-only.")

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  st.session_state.messages.append({"role": "assistant", "content": response_text})

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  st.session_state.awaiting_user_action = False

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  st.session_state.user_message_count += 1

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  st.rerun()



Â  Â  Â  Â  Â  Â  Â  Â  else:

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  st.session_state.chat_complete = True

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  st.rerun()

Â  Â  Â  Â  Â  Â  else:

Â  Â  Â  Â  Â  Â  Â  Â  st.warning("Please provide an answer before sending.")



Â  Â  Â  Â  if st.button("Finish Interview and Get Feedback", key="finish_interview_button"):

Â  Â  Â  Â  Â  Â  st.session_state.chat_complete = True

Â  Â  Â  Â  Â  Â  st.session_state.awaiting_user_action = False

Â  Â  Â  Â  Â  Â  st.rerun()



Â  Â  # Case 3: Interview is complete

Â  Â  else:

Â  Â  Â  Â  st.session_state.chat_complete = True





# --- Feedback and Restart ---

if st.session_state.chat_complete and not st.session_state.feedback_shown:

Â  Â  if st.button("Get Feedback", on_click=show_feedback, key="get_feedback_button"):

Â  Â  Â  Â  st.write("Fetching feedback...")



if st.session_state.feedback_shown:

Â  Â  st.subheader("Feedback")



Â  Â  feedback_messages_for_llm = [

Â  Â  Â  Â  {"role": msg["role"], "content": msg["content"]}

Â  Â  Â  Â  for msg in st.session_state.messages if msg["role"] != "system"

Â  Â  ]

Â  Â  conversation_history = "\n".join([f"{msg['role']}: {msg['content']}" for msg in feedback_messages_for_llm])



Â  Â  feedback_client = OpenAI(api_key=st.secrets["OPENAI_API_KEY"])



Â  Â  feedback_completion = feedback_client.chat.completions.create(

Â  Â  Â  Â  model="gpt-4o",

Â  Â  Â  Â  messages=[

Â  Â  Â  Â  Â  Â  {"role": "system", "content": """You are a helpful tool that provides feedback on an interviewee performance.

Â  Â  Â  Â  Â  Â  Before the Feedback give a score of 1 to 10.

Â  Â  Â  Â  Â  Â  Follow this format:

Â  Â  Â  Â  Â  Â  Overall Score: //Your score

Â  Â  Â  Â  Â  Â  Feedback: //Here you put your feedback

Â  Â  Â  Â  Â  Â  Give only the feedback do not ask any additional questions.

Â  Â  Â  Â  Â  Â  """},

Â  Â  Â  Â  Â  Â  {"role": "user", "content": f"This is the interview you need to evaluate. Keep in mind that you are only a tool. And you shouldn't engage in any conversation: {conversation_history}"}

Â  Â  Â  Â  ]

Â  Â  )



Â  Â  feedback_text = feedback_completion.choices[0].message.content

Â  Â  st.write(feedback_text)

Â  Â Â 

Â  Â  # Generate and autoplay audio of the feedback

Â  Â  feedback_audio_file = "feedback_summary.mp3"

Â  Â  try:

Â  Â  Â  Â  text_to_audio(feedback_client, feedback_text, feedback_audio_file, voice_type="alloy")

Â  Â  Â  Â  st.session_state.feedback_audio_path = feedback_audio_file

Â  Â  Â  Â  auto_play_audio(feedback_audio_file)

Â  Â  except Exception as e:

Â  Â  Â  Â  st.error(f"Error generating feedback audio: {e}. Displaying text only.")

Â  Â  Â  Â Â 

Â  Â  if st.button("Restart Interview", type="primary", key="restart_interview_button_final"):

Â  Â  Â  Â  # Clean up all audio files on restart

Â  Â  Â  Â  if st.session_state.feedback_audio_path and os.path.exists(st.session_state.feedback_audio_path):

Â  Â  Â  Â  Â  Â  os.remove(st.session_state.feedback_audio_path)

Â  Â  Â  Â  for message in st.session_state.messages:

Â  Â  Â  Â  Â  Â  if message.get("audio_file_path") and os.path.exists(message["audio_file_path"]):

Â  Â  Â  Â  Â  Â  Â  Â  os.remove(message["audio_file_path"])

Â  Â  Â  Â  streamlit_js_eval(js_expressions="parent.window.location.reload()")
